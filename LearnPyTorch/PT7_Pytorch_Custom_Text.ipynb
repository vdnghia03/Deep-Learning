{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport spacy\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom PIL import Image\nimport torchvision.transforms as transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:21:43.861002Z","iopub.execute_input":"2025-10-01T04:21:43.861311Z","iopub.status.idle":"2025-10-01T04:21:43.865450Z","shell.execute_reply.started":"2025-10-01T04:21:43.861289Z","shell.execute_reply":"2025-10-01T04:21:43.864856Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"we want to convert text -> numerical values\n 1. We need a Vocabulary mapping each word to a index\n 2. We need to setup a Pytorch dataset to load the data\n 3. Setup padding of every batch (all examples should be of same seq_len and setup dataloader)\n\nNote that loading the image is very easy compared to the text.","metadata":{}},{"cell_type":"code","source":"spacy_eng = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:20:41.394132Z","iopub.execute_input":"2025-10-01T04:20:41.394605Z","iopub.status.idle":"2025-10-01T04:20:42.088652Z","shell.execute_reply.started":"2025-10-01T04:20:41.394583Z","shell.execute_reply":"2025-10-01T04:20:42.088100Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, threshold):\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.threshold = threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenizer_eng(text):\n        return [ tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequencies:\n                    frequencies[word] = 1\n                else:\n                    frequencies[word] += 1\n\n                if frequencies[word] == self.threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n    def numericalize(self, text):\n        tokenized_text = self.tokenizer_eng(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:31:32.392498Z","iopub.execute_input":"2025-10-01T04:31:32.392976Z","iopub.status.idle":"2025-10-01T04:31:32.399194Z","shell.execute_reply.started":"2025-10-01T04:31:32.392955Z","shell.execute_reply":"2025-10-01T04:31:32.398367Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, root_dir, captions_file, transform, threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(captions_file)\n        self.transform = transform\n\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n\n        self.vocab = Vocabulary(threshold)\n        self.vocab.build_vocabulary(self.captions.to_list())\n\n\n    def __len__(self):\n        return len(self.captions)\n\n    def __getitem__(self, index):\n        image_id = self.imgs[index]\n        caption_id = self.captions[index]\n\n        image_path = os.path.join(self.root_dir, image_id)\n        image = Image.open(image_path).convert(\"RGB\")\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption_id)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return image, torch.tensor(numericalized_caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:35:43.394580Z","iopub.execute_input":"2025-10-01T04:35:43.394823Z","iopub.status.idle":"2025-10-01T04:35:43.400820Z","shell.execute_reply.started":"2025-10-01T04:35:43.394807Z","shell.execute_reply":"2025-10-01T04:35:43.400108Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"class MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim = 0)\n\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value = self.pad_idx)\n\n\n        return imgs, targets\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:39:17.640942Z","iopub.execute_input":"2025-10-01T04:39:17.641542Z","iopub.status.idle":"2025-10-01T04:39:17.647260Z","shell.execute_reply.started":"2025-10-01T04:39:17.641507Z","shell.execute_reply":"2025-10-01T04:39:17.646443Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def get_loader(\n    root_folder,\n    annotation_file,\n    transform,\n    batch_size=32,\n    num_workers=4,\n    shuffle=True,\n    pin_memory=True,\n):\n    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        pin_memory=pin_memory,\n        collate_fn=MyCollate(pad_idx=pad_idx),\n    )\n\n    return loader, dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:37:35.428292Z","iopub.execute_input":"2025-10-01T04:37:35.428549Z","iopub.status.idle":"2025-10-01T04:37:35.433249Z","shell.execute_reply.started":"2025-10-01T04:37:35.428533Z","shell.execute_reply":"2025-10-01T04:37:35.432466Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    transform = transforms.Compose(\n        [\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ]\n    )\n\n    loader, dataset = get_loader(\n        \"/kaggle/input/flickr8k/Images\", \"/kaggle/input/flickr8k/captions.txt\", transform=transform\n    )\n\n    for idx, (imgs, captions) in enumerate(loader):\n        print(imgs.shape)\n        print(captions.shape)\n        \n        if idx == 20:\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T04:39:40.702116Z","iopub.execute_input":"2025-10-01T04:39:40.702926Z","iopub.status.idle":"2025-10-01T04:39:43.601591Z","shell.execute_reply.started":"2025-10-01T04:39:40.702891Z","shell.execute_reply":"2025-10-01T04:39:43.600857Z"}},"outputs":[{"name":"stdout","text":"torch.Size([32, 3, 224, 224])\ntorch.Size([22, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([23, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([25, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([33, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([23, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([36, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([22, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([22, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([19, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([23, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([19, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([23, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([22, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([25, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([17, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([23, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([20, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([22, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([23, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([24, 32])\ntorch.Size([32, 3, 224, 224])\ntorch.Size([19, 32])\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}