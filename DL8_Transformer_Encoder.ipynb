{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtse7SqZn2lOtOj/TtKHje",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vdnghia03/Deep-Learning/blob/main/DL8_Transformer_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder"
      ],
      "metadata": {
        "id": "hdA4XGq55AmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vấn đề RNN\n",
        "\n",
        "Trong mô hình RNN nó:\n",
        "- Xử lí tuần tự\n",
        "- Mất thông tin của các từ đằng trước\n"
      ],
      "metadata": {
        "id": "oxOfCuYd582Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Logo](https://github.com/vdnghia03/Data_Source/blob/main/Transformer/ed1.png?raw=true)"
      ],
      "metadata": {
        "id": "GrUK3occ-CnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Người ta muốn tạo ra một từ mới mà nó củng chứa được thông tin của những từ khác, ví dụ x1 mang theo thông tin của x2,x3,x4 như ảnh trên.\n",
        "\n",
        "Người ta sẽ đẩy D0 -> Y0, D1 -> Y1, ...., Dn -> Yn với Y0, Y1,..,Yn là các tổ hợp tuyến tính từ {D0, D1, ..., Dn}\n",
        "\n",
        "Ở đây các hệ số a00, a01, ..., a0n là các trọng số, có được từ tích vô hướng, ví dụ a00 = D0 . D0, a01 = D0 . D1, ...\n",
        "\n",
        "![](https://github.com/vdnghia03/Data_Source/blob/main/Transformer/ed2.png?raw=true)"
      ],
      "metadata": {
        "id": "UJx1CDX69_I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nền tảng Attension"
      ],
      "metadata": {
        "id": "fBpojZb_Bdfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Desire Properties**\n",
        "  - ![](https://github.com/vdnghia03/Data_Source/blob/main/Transformer/ed3.png?raw=True)\n",
        "\n",
        "  - ![](https://github.com/vdnghia03/Data_Source/blob/main/Transformer/ed4.png?raw=True)"
      ],
      "metadata": {
        "id": "MmKPQlZrC5bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Context Awareness**\n",
        "  - ![](https://github.com/vdnghia03/Data_Source/blob/main/Transformer/ed6.png?raw=True)"
      ],
      "metadata": {
        "id": "Elk0lWbsDq1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax Scaler**:\n",
        "- 0<value<1\n",
        "- Sum(value) = 1"
      ],
      "metadata": {
        "id": "ZM_9iOA4FRGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention with Q-K-V\n",
        "\n",
        "Ta thấy với Attension:\n",
        "$$\n",
        "\\alpha = \\text{softmax}(\\frac{DD^T}{\\sqrt{m}})\n",
        "$$\n",
        "\n",
        "$$\n",
        "Y = \\text{softmax}(\\frac{DD^T}{\\sqrt{m}})D\n",
        "$$\n",
        "\n",
        "![](https://github.com/vdnghia03/Data_Source/blob/main/Transformer/ed7.png?raw=True)\n",
        "\n",
        "Các hệ số tương quan α nó bị fix cứng, người ta muốn nó là param cần thay đổi thông qua quá trình học.\n",
        "Ví dụ với câu `I love this Movie` thì hệ số tương quan của từ I (Query) với các từ còn lại là  α =  (0.5, 0.3, 0.1, 0.1) (Key) còn V (Value Embedding)n.\n",
        "\n",
        "Người ta muốn cập nhật α qua quá trình học để nó học được các đặc trưng ngữ cảnh tốt nhất. Bằng cách sử dụng các ma trận:\n",
        "- W_Q -> Q = X . W_Q (X tương đương D)\n",
        "- W_K -> K = X . W_K\n",
        "- W_V -> V = X . W_V\n",
        "\n",
        "Cập nhật qua quá trình học sẽ được hệ số tương quan tốt nhất, nó chính là cái ta cần cập nhật khi Backprobagation\n",
        "\n",
        "$$\n",
        "Y = \\alpha V = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "![](https://github.com/vdnghia03/Data_Source/blob/main/Transformer/ed9.png?raw=True)"
      ],
      "metadata": {
        "id": "Q-_XCOw_HcQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi Head Attention\n",
        "\n",
        "![](https://github.com/vdnghia03/Data_Source/blob/main/Transformer/ed12.png?raw=True)\n",
        "\n",
        "![](https://github.com/vdnghia03/Data_Source/blob/main/Transformer/ed13.png?raw=True)"
      ],
      "metadata": {
        "id": "x69y5L4qJlY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "\n",
        "**Positioning Embedding**\n",
        "\n",
        "**Multi head attention**\n",
        "\n",
        "**Add and Norm**\n",
        "\n",
        "![](https://github.com/vdnghia03/Data_Source/blob/main/Transformer/ed14.png?raw=True)"
      ],
      "metadata": {
        "id": "NeE5trX_eiT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Feed Forward**:\n",
        "\n",
        "Cấu trúc của Lớp Feed Forward\n",
        "\n",
        "Nó là một mạng nơ-ron 2 lớp rất đơn giản. Nó bao gồm:\n",
        "\n",
        "    Lớp tuyến tính 1 (Expansion): Mở rộng kích thước của vector.\n",
        "\n",
        "    Hàm kích hoạt (Activation): Thường là ReLU hoặc GELU (thêm tính phi tuyến).\n",
        "\n",
        "    Lớp tuyến tính 2 (Contraction): Thu hẹp kích thước vector trở lại như ban đầu.\n",
        "\n"
      ],
      "metadata": {
        "id": "f3jTKiKUeiRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## IBERTMDB_Tranformer_1Layer_BERT"
      ],
      "metadata": {
        "id": "Wss70ZTnkFps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phân loại (ví dụ: \"Tích cực\" hay \"Tiêu cực\").\n",
        "\n",
        "Chúng ta sẽ dùng một câu ví dụ: `\"Bộ phim này rất hay.\"`\n",
        "Và giả sử chúng ta phân loại thành 2 lớp: **0 (Tiêu cực)** và **1 (Tích cực)**.\n",
        "Giả sử $d_{model}$ (kích thước vector) của chúng ta là 768.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Giai đoạn 1: Chuẩn bị Đầu vào (Biến chữ thành số)\n",
        "\n",
        "Mục tiêu của giai đoạn này là biến câu văn `\"Bộ phim này rất hay.\"` thành một ma trận (danh sách các vector) có chứa cả **ý nghĩa** và **vị trí**.\n",
        "\n",
        "### Bước 1: Tiền xử lý & Tokenize\n",
        "Câu văn được \"dọn dẹp\" và \"băm\" nhỏ thành các token. Quan trọng nhất, một token đặc biệt là `[CLS]` (viết tắt của Classification) được thêm vào đầu câu.\n",
        "\n",
        "* **Đầu vào:** `\"Bộ phim này rất hay.\"`\n",
        "* **Đầu ra:** `['[CLS]', 'bộ', 'phim', 'này', 'rất', 'hay', '.']`\n",
        "\n",
        "### Bước 2: Input Embedding (Vector Ý nghĩa)\n",
        "Mỗi token được tra cứu trong một \"từ điển\" (bảng Embedding) để lấy vector *ý nghĩa* của nó.\n",
        "\n",
        "* `'[CLS]'` $\\rightarrow$ `Vector_nghĩa_CLS` (ví dụ: `[0.1, 0.9, 0.2, ...]` 768 chiều)\n",
        "* `'bộ'` $\\rightarrow$ `Vector_nghĩa_bộ` (ví dụ: `[0.5, 0.1, 0.7, ...]` 768 chiều)\n",
        "* ...\n",
        "* `'.'` $\\rightarrow$ `Vector_nghĩa_chấm` (ví dụ: `[0.3, 0.3, 0.5, ...]` 768 chiều)\n",
        "\n",
        "### Bước 3: Positional Embedding (Vector Vị trí)\n",
        "Song song đó, mô hình tạo ra một vector *địa chỉ* cho từng vị trí (dùng công thức `sin`/`cos` hoặc học được).\n",
        "\n",
        "* Vị trí 0 $\\rightarrow$ `Vector_vị_trí_0` (ví dụ: `[0.0, 1.0, 0.0, ...]` 768 chiều)\n",
        "* Vị trí 1 $\\rightarrow$ `Vector_vị_trí_1` (ví dụ: `[0.8, 0.5, 0.0, ...]` 768 chiều)\n",
        "* ...\n",
        "* Vị trí 6 $\\rightarrow$ `Vector_vị_trí_6` (ví dụ: `[0.9, 0.1, 0.2, ...]` 768 chiều)\n",
        "\n",
        "### Bước 4: Phép cộng đầu vào\n",
        "Mô hình **cộng** vector ý nghĩa và vector vị trí tương ứng lại với nhau.\n",
        "\n",
        "* **Vector 0 (cho `[CLS]`):** `Vector_nghĩa_CLS` + `Vector_vị_trí_0`\n",
        "* **Vector 1 (cho `'bộ'`):** `Vector_nghĩa_bộ` + `Vector_vị_trí_1`\n",
        "* ...\n",
        "* **Vector 6 (cho `'.'`):** `Vector_nghĩa_chấm` + `Vector_vị_trí_6`\n",
        "\n",
        "**Kết quả Giai đoạn 1:** Chúng ta có một ma trận (danh sách 7 vector) kích thước $7 \\times 768$. Ma trận này là đầu vào cho khối Encoder đầu tiên.\n",
        "\n",
        "---\n",
        "\n",
        "## Giai đoạn 2: Lõi Encoder (Học ngữ cảnh)\n",
        "\n",
        "Đây là trái tim của mô hình. Dữ liệu sẽ đi qua một \"chồng\" (Stack) gồm N khối Encoder (ví dụ, BERT-base có N=12). Dưới đây là những gì xảy ra **bên trong 1 khối Encoder**:\n",
        "\n",
        "### Bước 5: Multi-Head Attention (Trao đổi thông tin)\n",
        "Ma trận $7 \\times 768$ từ Giai đoạn 1 được đưa vào đây.\n",
        "\n",
        "* **Quá trình:** Mỗi vector (mỗi từ) sẽ \"nhìn\" vào 6 vector còn lại (và chính nó) để tính toán \"mức độ liên quan\". Token `[CLS]` sẽ \"học\" từ tất cả các token khác.\n",
        "* **Đầu ra:** Một ma trận $7 \\times 768$ **mới**. Các vector ở đây đã được \"trộn\" thông tin với nhau.\n",
        "    * Ví dụ: Vector của từ `'hay'` giờ đây đã \"biết\" rằng nó đang bổ nghĩa cho `'bộ phim'`, chứ không phải một thứ gì khác. Vector `[CLS]` bắt đầu \"tóm tắt\" ý nghĩa chung.\n",
        "\n",
        "### Bước 6: Add & Norm (Lần 1)\n",
        "* **Add:** Lấy kết quả từ Bước 5 cộng với đầu vào (trước khi vào Attention). Đây gọi là \"Residual Connection\" (kết nối tắt), giúp mô hình không bị \"quên\" thông tin gốc.\n",
        "* **Norm:** \"Chuẩn hóa\" (Layer Normalization) kết quả để giữ cho các giá trị ổn định.\n",
        "\n",
        "### Bước 7: Feed Forward Network (FFN) (Tiêu hóa thông tin)\n",
        "Kết quả từ Bước 6 được đưa vào đây.\n",
        "\n",
        "* **Quá trình:** Mỗi vector trong 7 vector sẽ đi qua một mạng nơ-ron 2 lớp *một cách độc lập* (như bạn đã hỏi ở trên).\n",
        "* **Mục đích:** Giúp mô hình \"suy nghĩ\" hoặc \"tiêu hóa\" thông tin ngữ cảnh mà nó vừa học được từ lớp Attention.\n",
        "\n",
        "### Bước 8: Add & Norm (Lần 2)\n",
        "Tương tự Bước 6:\n",
        "* **Add:** Lấy kết quả từ Bước 7 cộng với đầu vào (trước khi vào FFN).\n",
        "* **Norm:** Chuẩn hóa lần nữa.\n",
        "\n",
        "**Kết thúc 1 khối Encoder:** Chúng ta thu được một ma trận $7 \\times 768$ \"thông minh hơn\".\n",
        "\n",
        "### Bước 9: Lặp lại N lần\n",
        "Ma trận $7 \\times 768$ này sẽ trở thành **đầu vào** cho khối Encoder thứ 2, và quy trình (Bước 5 $\\rightarrow$ 8) lặp lại. Quá trình này lặp đi lặp lại N lần (ví dụ: 12 lần).\n",
        "\n",
        "**Kết quả Giai đoạn 2:** Sau khi đi qua N khối Encoder, chúng ta có một ma trận $7 \\times 768$ cuối cùng. Các vector ở đây đã \"siêu thông minh\" và hiểu rất sâu về ngữ cảnh của câu.\n",
        "\n",
        "---\n",
        "\n",
        "## Giai đoạn 3: Phân loại (Đưa ra quyết định)\n",
        "\n",
        "Mục tiêu của giai đoạn này là biến ma trận $7 \\times 768$ \"thông minh\" thành một dự đoán duy nhất (\"Tích cực\" hay \"Tiêu cực\").\n",
        "\n",
        "### Bước 10: Lấy Vector [CLS]\n",
        "Mô hình **bỏ qua tất cả 6 vector sau**, và **chỉ lấy vector đầu tiên** (vector tương ứng với token `[CLS]` ở vị trí 0).\n",
        "\n",
        "* **Tại sao?** Vì trong suốt Giai đoạn 2, token `[CLS]` đã được huấn luyện để \"hấp thụ\" và \"tóm tắt\" toàn bộ ý nghĩa của câu. Nó chính là \"bản tóm tắt\" của câu.\n",
        "* **Đầu vào cho bước tiếp:** Một vector duy nhất $C$ (kích thước $1 \\times 768$).\n",
        "\n",
        "### Bước 11: Lớp Linear (Phân loại)\n",
        "Vector $C$ ($1 \\times 768$) được đưa qua một lớp Linear (hay còn gọi là Dense) cuối cùng.\n",
        "\n",
        "* **Mục đích:** \"Ép\" vector 768 chiều xuống còn $K$ chiều (với $K$ là số lớp).\n",
        "* **Trong ví dụ này ($K=2$):** Lớp Linear sẽ biến vector $1 \\times 768$ thành vector $1 \\times 2$.\n",
        "* **Đầu ra (Logits):** Một vector điểm thô, ví dụ: `[-1.2, 2.5]`\n",
        "    * Điểm cho lớp 0 (Tiêu cực): -1.2\n",
        "    * Điểm cho lớp 1 (Tích cực): 2.5\n",
        "\n",
        "### Bước 12: Lớp Softmax (Chuyển thành Xác suất)\n",
        "Mô hình áp dụng hàm Softmax cho vector Logits để biến chúng thành xác suất (tổng bằng 1).\n",
        "\n",
        "* **Đầu vào:** `[-1.2, 2.5]`\n",
        "* **Đầu ra (Xác suất):** `[0.02, 0.98]`\n",
        "    * Xác suất là \"Tiêu cực\": 2%\n",
        "    * Xác suất là \"Tích cực\": 98%\n",
        "\n",
        "### Bước 13: Kết quả cuối cùng\n",
        "Mô hình chọn lớp có xác suất cao nhất.\n",
        "\n",
        "**Dự đoán: Lớp 1 (Tích cực).**"
      ],
      "metadata": {
        "id": "usKBCbP4q8P9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49wpAWaV24kz"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb_v1_extracted/aclImdb')\n",
        "os.listdir(dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4JyuXa4kxUN",
        "outputId": "db939af4-e8b7-4935-a97f-1bed41059be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imdbEr.txt', 'README', 'imdb.vocab', 'train', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "id": "8Rh9ProNlMit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb702c71-74f6-4a52-9fdc-31d33e6283f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unsup',\n",
              " 'urls_pos.txt',\n",
              " 'urls_unsup.txt',\n",
              " 'neg',\n",
              " 'unsupBow.feat',\n",
              " 'pos',\n",
              " 'urls_neg.txt',\n",
              " 'labeledBow.feat']"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "seed = 12345\n",
        "\n",
        "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb_v1_extracted/aclImdb/train', batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training', seed=seed)\n",
        "\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb_v1_extracted/aclImdb/train', batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation', seed=seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk5i9f8hoMr8",
        "outputId": "34e2ae8b-ebc5-467a-8b66-bd5550aceff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 75000 files belonging to 3 classes.\n",
            "Using 60000 files for training.\n",
            "Found 75000 files belonging to 3 classes.\n",
            "Using 15000 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in train_ds:\n",
        "    print(label_batch)\n",
        "    print(text_batch)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl8-YCE1x7u4",
        "outputId": "745f67b7-a353-464b-d1db-e2c628413dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([2 2 2 ... 2 2 2], shape=(1024,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[b'Lana Turner is young and gorgeous in this light comedy about a waitress with a lucky penny and the dream of getting out of her humdrum life. The penny works wonders. After escaping her small town, a ladder falls on her in New York City - when she comes to, it\\'s assumed she has amnesia (she was deciding on her new name when the ladder came crashing down). After a trip to the library, she decides to become a long-lost heiress who disappeared as a child in 1925.<br /><br />Robert Young plays her ex-boss who looks to expose her lie; Walter Brennan is her welcoming, wealthy father; Ward Bond is his security guy; Dame May Witty the heiress\\' former nanny. Ray Collins of \"Perry Mason\" fame also makes an appearance, as does Eugene Palette as the newspaper editor who prints the amnesiac\\'s story. It\\'s a terrific cast, with Young\\'s role being a departure for him. He does it well. Lana is simply adorable.<br /><br />The movie leaves an open question, which is kind of fun, too. All in all, very enjoyable.'\n",
            " b'Youth Danny Embling (Noah Taylor) suffers the pain of learning his long -time favourite piece of crumpet is starting to expand her own social circle, leaving him still too slow in the growth cycle to keep in step. Noah, Bruce Spence & Graeme Blundell are superb (with some thanks to flawless script and damn fine direction), while the bit parts played by Robert Carlton (only seen elsewhere in High Tide), are bonzah-funny. Beware, this feature plays on true-to -life reaction to true-to-life occurances, utilising true-to-life dialogue. The Year is an anti-soap. Conversation including \"I\\'m sorry, you\\'re so mature for your age.\", and \"He thinks you\\'re sexy\" would not suit if the film wasn\\'t meant to be aimed at describing ordinariness.'\n",
            " b'Sly looked cool and tough in the trailer and the poster and the cover of the video box. He\\'s a little girlie man in the movie. This is one of the worst \"action\" movies ever made. The director didn\\'t know how to move it along. It reminded me of a cheap movie called \"the swap\" which was called something else and had footage of a young Robert DeNiro in it. You know the movie I\\'m talking about, you saw it at Suncoast for $6.99 and almost bought it because it had a big picture of Robert DeNiro on it. THAT movie is better than this.<br /><br />Really, I\\'m not kidding. DO NOT SEE THIS MOVIE. <br /><br />'\n",
            " ...\n",
            " b\"Excellent action and fight scenes plus the women are all ultra hot. What more could you ask for?!! I saw a dubbed version. I am in search of a copy with subtitles but can't seem to find one. I prefer to hear the original Chinese spoken. Anyway, I loved the movie. It makes me want to find other titles featuring the three lead performers. I loved the kiss at the end. It was touching yet erotic at the same time. The plot was a little confusing at first but I finally caught on. All in all I would say the movie pleasantly surprised me. I would definitely recommend it. If you like hot babes kicking some serious ass you should see this movie.\"\n",
            " b\"Umberto Lenzi tried his hand at just about every popular type of film in Italy during the sixties, seventies and eighties, and while he has some very nice entries on his list of directorial credits; it has to be said that he isn't the best director to try his hand at the likes of Giallo, zombie and cannibal movies. However, when it comes to crime films; Lenzi comes into his own, and just like Almost Human a couple of years earlier, Violent Naples is an entertaining and nasty little crime thriller. Like every film in this sub-genre, the major influence comes from the masterpiece American film 'Dirty Harry', and here we follow a copper not too far removed from Clint Eastwood's famous character. Commissario Betti is your typical disgruntled cop that favours violence over proper police procedure. He's fed up with the way that Naples is being run by the criminals, and so sets his sights on The Commandante. However, this man isn't easy to bring down, and since Betti is receiving no help from his fellow townsmen, the task of ending crime in Naples is left to our leading man.<br /><br />The title of the film leads the viewer to expect lots of violence, and Lenzi certainly doesn't disappoint in this respect. We've got bowling balls to the head, a young woman having her face scraped off on a moving train, a man impaled on a sharp fence and many other such delights. Overall, it has to be said that this film isn't very original, but Lenzi injects some freshness into it with a barrage of exciting chase scenes (the best of which take a point of view shot from the front of a motorbike) and violent shootouts. The good thing about these Polizia movies is that they are played out for pure entertainment value, meaning that Lenzi is free from trying to be arty, and can pack in as much violence as he likes; a fact which is often capitalised on. The film benefits from a strong Italian cast, including Maurizio Merli; who may be no Clint Eastwood, but still delivers a strong and entertaining lead performance. John Saxon gets to join in the fun also, and there's also a place for fellow American actor Barry Sullivan, in the role of the villain. Overall, if you're into Italian cop movies; this isn't one to miss!\"\n",
            " b'Terrible... Pathetic...<br /><br />If anything, it is now obvious that the cursed city of Sarajevo (and its authors like Sidran) made Kusturica, once upon a time.<br /><br />Without the inspiration from there, he is forced to imitate himself -- as someone else pointed out. If we forget his American adventure (Arizona Dream), the decay of his movies was apparent already when \"Underground\" came out, but it was still bearable. \"Black cat, white cat\" was still watchable, though clearly worse than the rest. This one is simply pathetic.<br /><br />Rest In Peace, Emir of Dolly Bell...'], shape=(1024,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in train_ds:\n",
        "  print(label_batch[0].numpy())\n",
        "  print(text_batch.numpy()[0].decode('ascii'))\n",
        "  break"
      ],
      "metadata": {
        "id": "NQNYY1V7yjUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8658d0b1-9225-44f4-8bd2-ce43ce88bddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "In this ass kicking, mother eating high tech action film the former king of Hollywood once again gives a water proof performance in the role of Jack Carter. The film is a full blooded hit in the mix of excitement and sensibility and I wish and hope that everyone goes and sees it!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Định nghĩa các hằng số\n",
        "vocab_size = 20000\n",
        "sequence_len = 200\n",
        "\n",
        "# 2. Định nghĩa hàm chuẩn hóa văn bản tùy chỉnh\n",
        "def custom_standardization(input_data):\n",
        "    # Chuyển văn bản về chữ thường\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "\n",
        "    # Xóa thẻ HTML <br /> (thường gặp trong các bài đánh giá)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "\n",
        "    # Xóa tất cả các dấu câu\n",
        "    # re.escape(string.punctuation) tạo ra một chuỗi regex an toàn\n",
        "    # ví dụ: '[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "# 3. Khởi tạo lớp TextVectorization\n",
        "vectorization = tf.keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,  # Sử dụng hàm chuẩn hóa ở trên\n",
        "    max_tokens=vocab_size,             # Giới hạn 20,000 từ vựng\n",
        "    output_mode='int',                 # Xuất ra chuỗi số nguyên\n",
        "    output_sequence_length=sequence_len, # Đồng nhất độ dài chuỗi là 200\n",
        ")\n",
        "\n",
        "# 4. \"Huấn luyện\" lớp vectorization trên dữ liệu training\n",
        "# (Giả sử bạn có một tf.data.Dataset tên là train_ds)\n",
        "vectorization.adapt(train_ds.map(lambda text, label: text))"
      ],
      "metadata": {
        "id": "KWyQTzH3Jk3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = custom_standardization(text_batch[0])\n",
        "# Dòng print trong ảnh:\n",
        "print(output.numpy().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dfl9alb9PYik",
        "outputId": "67e2d9e2-d5b5-4dec-814b-60a12f1403b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in this ass kicking mother eating high tech action film the former king of hollywood once again gives a water proof performance in the role of jack carter the film is a full blooded hit in the mix of excitement and sensibility and i wish and hope that everyone goes and sees it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Khối 2: Định nghĩa hàm vector hóa và áp dụng nó vào dataset\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorization(text), label\n",
        "\n",
        "# Áp dụng hàm này cho cả tập huấn luyện và tập validation\n",
        "train_ds = train_ds.map(vectorize_text)\n",
        "val_ds = val_ds.map(vectorize_text)"
      ],
      "metadata": {
        "id": "jUhoE9xJRYEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Khối 3: Kiểm tra một lô (batch) dữ liệu sau khi vector hóa\n",
        "for text_batch, label_batch in train_ds:\n",
        "    print(label_batch[0].numpy())\n",
        "    print(text_batch[0].numpy())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DECuCHvYRzZ6",
        "outputId": "45b02f53-262d-45bd-a7a0-5fd767784bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "[   38   360    11    25   109     3  6629    86     5     2  1143     3\n",
            "    91  3855     2 13654  3116   580     4   166   904   509 12942   433\n",
            "    20     2  1612     3     1     5     2  8256     6     2   792   580\n",
            " 14484   305    14     6 14484    11   576  3182    87    16     1     1\n",
            "  4510  7631 12708     1     3 10026    35    55     2  4090     1    11\n",
            "   255  7662     3  1991 14484  7633    16    24   300    61  2477    16\n",
            " 14484   104  2498     1     3  5912  4712    59   219   411    25 10979\n",
            "     4  1106  4661  4090    16  4510  1336    55   471     2   167     1\n",
            "  2548 14484   305    97   108   953   153   664  5559   164    99     8\n",
            "  1132   542   901     3  3448     8    57   652    27  9582  7698     3\n",
            "  3520     2  1717  5912  4712     6  1581     2  2122   272    27  2477\n",
            "    16    24     1     6    87   189   336    68     6   762    16    24\n",
            "   300     6  3156     2   300    68     6  4275   382    32  4343     9\n",
            "    13    42     2   193     5  5912  4712    24  1612     3  2891  6847\n",
            "    12     1  1141     1   300     7    21   816     9     7    21  7517\n",
            "    41     1     9     7   462     3  2497     1    15    24   191 13450\n",
            "  3938  1631    48    86    79  1187     2 10226]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds   = val_ds.cache().prefetch(buffer_size=10)"
      ],
      "metadata": {
        "id": "bwGpcWQmR52q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "hzKajniYSeG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "8K2pzGF-SWCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Chú thích trong ảnh: Hai lớp embedding riêng biệt, một cho token,\n",
        "# một cho chỉ số token (vị trí)\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_emb = layers.Embedding(\n",
        "            input_dim=vocab_size,  # Kích thước từ vựng (ví dụ: 20000)\n",
        "            output_dim=embed_dim   # Kích thước vector embedding (ví dụ: 128)\n",
        "        )\n",
        "\n",
        "        self.pos_emb = layers.Embedding(\n",
        "            input_dim=maxlen,      # Độ dài tối đa của chuỗi (ví dụ: 200)\n",
        "            output_dim=embed_dim   # Kích thước cũng phải là 128\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "4gWBz16KZ6O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "embed_dim = 128    # Embedding size for each token\n",
        "num_heads = 6      # Number of attention heads\n",
        "ff_dim = 512       # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(sequence_len,))\n",
        "embedding_layer = TokenAndPositionEmbedding(sequence_len, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "-bUtFla7Z7Ux",
        "outputId": "0e0189a6-713b-47af-8c35-5bbf0b56d60b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_21\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_21\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_23 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding_13 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m2,585,600\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_12            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m527,872\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_10     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_57 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_58 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m99\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding_13 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,585,600</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_12            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,872</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_10     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,117,699\u001b[0m (11.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,117,699</span> (11.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,117,699\u001b[0m (11.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,117,699</span> (11.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_ds, batch_size=32, epochs=5, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_6PFdmfa_SG",
        "outputId": "008662b3-bff8-4c02-c15a-a2ce76a5c1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 619ms/step - accuracy: 0.6438 - loss: 0.9034 - val_accuracy: 0.6591 - val_loss: 0.8716\n",
            "Epoch 2/5\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 508ms/step - accuracy: 0.6717 - loss: 0.8459 - val_accuracy: 0.6591 - val_loss: 0.7506\n",
            "Epoch 3/5\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 523ms/step - accuracy: 0.6729 - loss: 0.7122 - val_accuracy: 0.6785 - val_loss: 0.6907\n",
            "Epoch 4/5\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 534ms/step - accuracy: 0.7475 - loss: 0.5518 - val_accuracy: 0.6925 - val_loss: 0.7329\n",
            "Epoch 5/5\n",
            "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 544ms/step - accuracy: 0.7917 - loss: 0.4568 - val_accuracy: 0.6420 - val_loss: 0.8742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rc0mMqsEc0t_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}